{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-24 03:42:18,777] Making new env: ppaquette/DoomDefendLine-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from ppaquette_gym_doom.wrappers import observation_space\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "env = gym.make('ppaquette/DoomDefendLine-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from doom_py import ScreenResolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.choice([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "env = gym.make('ppaquette/DoomDefendLine-v0')\n",
    "env.reset()\n",
    "for _ in range(5000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrapper = observation_space.SetResolution('320x240')\n",
    "env = wrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 107, 3)\n",
      "[ 85.87373235  65.64068403  47.65937562]\n",
      "[ 88.24537681  61.66036985  47.70888845]\n",
      "[ 102.05448399   72.32869358   58.99244383]\n",
      "[ 85.94571485  60.28454961  46.4472062 ]\n",
      "[ 86.07522198  65.82124309  47.79745351]\n"
     ]
    }
   ],
   "source": [
    "# THIS IS FOR 5 FRAMES\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(1):\n",
    "        env.render()\n",
    "        # print(observation.shape)\n",
    "        action = [0]*43\n",
    "        action[14] = 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        frames = observation[188:235, 40:600]\n",
    "        frame1 = frames[:, 1:108]\n",
    "        frame2 = frames[:, 109:216]\n",
    "        frame3 = frames[:, 217:324]\n",
    "        frame4 = frames[:, 325:432]\n",
    "        frame5 = frames[:, 433:600]\n",
    "        rgb_frame1 = np.mean(frame1, axis=(0, 1))\n",
    "        rgb_frame2 = np.mean(frame2, axis=(0, 1))\n",
    "        rgb_frame3 = np.mean(frame3, axis=(0, 1))\n",
    "        rgb_frame4 = np.mean(frame4, axis=(0, 1))\n",
    "        rgb_frame5 = np.mean(frame5, axis=(0, 1))\n",
    "        print(frame1.shape)\n",
    "        print(rgb_frame1)\n",
    "        print(rgb_frame2)\n",
    "        print(rgb_frame3)\n",
    "        print(rgb_frame4)\n",
    "        print(rgb_frame5)\n",
    "        plt.imshow(frame1)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actionToAngle(action):\n",
    "    return {\n",
    "        'right': 3,\n",
    "        'left': -3,\n",
    "        'shoot': 0\n",
    "    }.get(action, 0)  # 0 default if action not found \n",
    "\n",
    "def angleToState(angle):\n",
    "    return {\n",
    "        0: 1,\n",
    "        3: 1,\n",
    "        6: 2,\n",
    "        9: 2,\n",
    "        12: 3,\n",
    "        15: 3,\n",
    "        18: 4,\n",
    "        21: 4,\n",
    "        24: 5,\n",
    "        27: 5,\n",
    "        30: 6,\n",
    "        -3: 1,\n",
    "        -6: 7,\n",
    "        -9: 7,\n",
    "        -12: 8,\n",
    "        -15: 8,\n",
    "        -18: 9,\n",
    "        -21: 9,\n",
    "        -24: 10,\n",
    "        -27: 10,\n",
    "        -30: 11,\n",
    "    }.get(angle, 0)\n",
    "\n",
    "def next_possible_states(current_state):\n",
    "    return {\n",
    "        1: (2,7),\n",
    "        2: (1,3),\n",
    "        3: (2,4),\n",
    "        4: (3,5),\n",
    "        5: (4,6),\n",
    "        6: (5,0),\n",
    "        0: (6,11),\n",
    "        11: (0,10),\n",
    "        10: (11,9),\n",
    "        9: (10,8),\n",
    "        8: (9,7),\n",
    "        7: (8,1)\n",
    "    }.get(current_state, 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(next_possible_states(2))\n",
    "angleToState(9)\n",
    "x = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.index(rd.choice(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frameToFind(a):\n",
    "#     print(a)\n",
    "    a = a.tolist()\n",
    "    if a[0]>100:\n",
    "        return 1\n",
    "    elif a[0] < 96 and a[0] > 79:\n",
    "        return 0\n",
    "    elif a[0] < 78 and a[0] > 73:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 675 timesteps\n",
      "[[ 0.0985857   0.09833824  0.0760416 ]\n",
      " [ 0.05080511  0.00663612  0.00604787]\n",
      " [ 0.02832002  0.03392641  0.00457246]\n",
      " [ 0.00580142  0.0028043   0.00305338]\n",
      " [ 0.01791882  0.00900804  0.01711527]\n",
      " [ 0.08982364  0.04859781  0.11718802]\n",
      " [ 0.05900306  0.02928205  0.10923479]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "[13.0]\n",
      "Episode finished after 537 timesteps\n",
      "[[ 0.20319656  0.22376386  0.18184122]\n",
      " [ 0.15827252  0.01687403  0.01070019]\n",
      " [ 0.04414179  0.08544275  0.05768017]\n",
      " [ 0.19794045  0.26669114  0.20808512]\n",
      " [ 0.26542041  0.24760413  0.26733573]\n",
      " [ 0.2525614   0.30121896  0.21689024]\n",
      " [ 0.20798342  0.2187669   0.24989826]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "[13.0, 15.0]\n",
      "Episode finished after 533 timesteps\n",
      "[[ 0.20319656  0.22387832  0.18614794]\n",
      " [ 0.31888567  0.24713187  0.24442723]\n",
      " [ 0.33394902  0.2571474   0.25300095]\n",
      " [ 0.32704259  0.28901204  0.29055339]\n",
      " [ 0.39307446  0.31070388  0.32682543]\n",
      " [ 0.2598196   0.30129268  0.23395982]\n",
      " [ 0.21464637  0.23888416  0.26777495]\n",
      " [ 0.20079497  0.10359422  0.16934943]\n",
      " [ 0.04083929  0.01318215  0.01033565]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "[13.0, 15.0, 13.0]\n",
      "Episode finished after 748 timesteps\n",
      "[[ 0.31721121  0.24553976  0.31845887]\n",
      " [ 0.31391782  0.2524741   0.24442723]\n",
      " [ 0.32642299  0.2571474   0.25713469]\n",
      " [ 0.39067691  0.29493232  0.30534575]\n",
      " [ 0.37586045  0.35571913  0.36769286]\n",
      " [ 0.41033606  0.34089256  0.34354398]\n",
      " [ 0.35314005  0.32796258  0.32465258]\n",
      " [ 0.20079497  0.10359422  0.16934943]\n",
      " [ 0.04083929  0.01318215  0.01033565]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "[13.0, 15.0, 13.0, 11.0]\n",
      "Episode finished after 788 timesteps\n",
      "[[ 0.24780347  0.31627796  0.31621583]\n",
      " [ 0.32163172  0.28050949  0.28236894]\n",
      " [ 0.37505601  0.34852956  0.32580857]\n",
      " [ 0.40140316  0.30538185  0.31648396]\n",
      " [ 0.38109899  0.37030361  0.36769286]\n",
      " [ 0.3663148   0.34179629  0.34406322]\n",
      " [ 0.44899567  0.33266211  0.32709118]\n",
      " [ 0.24263189  0.13773636  0.19055605]\n",
      " [ 0.13546756  0.21307841  0.1143135 ]\n",
      " [ 0.21687219  0.16615649  0.15560342]\n",
      " [ 0.22827763  0.25030695  0.25751845]\n",
      " [ 0.28410483  0.24753413  0.23733236]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0]\n",
      "Episode finished after 652 timesteps\n",
      "[[ 0.39149569  0.42980246  0.40922452]\n",
      " [ 0.35314415  0.29508945  0.31418726]\n",
      " [ 0.45231806  0.35290854  0.33254488]\n",
      " [ 0.50406602  0.32470569  0.32554419]\n",
      " [ 0.41081321  0.36881194  0.36769286]\n",
      " [ 0.39528238  0.37108636  0.37395089]\n",
      " [ 0.3203192   0.41339001  0.42712077]\n",
      " [ 0.24263189  0.13773636  0.19055605]\n",
      " [ 0.13546756  0.21307841  0.1143135 ]\n",
      " [ 0.21687219  0.16615649  0.15560342]\n",
      " [ 0.22827763  0.25030695  0.25751845]\n",
      " [ 0.28410483  0.24753413  0.23733236]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0]\n",
      "Episode finished after 645 timesteps\n",
      "[[ 0.40605363  0.42623442  0.41323696]\n",
      " [ 0.46169484  0.31636884  0.32347716]\n",
      " [ 0.45268157  0.36298363  0.35555664]\n",
      " [ 0.48570203  0.32470569  0.33373111]\n",
      " [ 0.43868161  0.40891619  0.40819082]\n",
      " [ 0.42957398  0.37567943  0.44328314]\n",
      " [ 0.42287713  0.51906013  0.41746916]\n",
      " [ 0.24263189  0.13773636  0.19055605]\n",
      " [ 0.13546756  0.21307841  0.1143135 ]\n",
      " [ 0.21687219  0.16615649  0.15560342]\n",
      " [ 0.22827763  0.25030695  0.25751845]\n",
      " [ 0.28410483  0.24753413  0.23733236]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0]\n",
      "Episode finished after 889 timesteps\n",
      "[[ 0.39843746  0.39952085  0.30183749]\n",
      " [ 0.4355402   0.3254733   0.33187078]\n",
      " [ 0.45953882  0.36298363  0.35555664]\n",
      " [ 0.52066803  0.34763612  0.35638655]\n",
      " [ 0.4855166   0.47936325  0.4770961 ]\n",
      " [ 0.43937957  0.4402456   0.50552043]\n",
      " [ 0.44781242  0.42133079  0.43122558]\n",
      " [ 0.2729662   0.13773636  0.19055605]\n",
      " [ 0.1864148   0.21806157  0.17366251]\n",
      " [ 0.24358923  0.20735684  0.20230934]\n",
      " [ 0.24441976  0.25614088  0.27300452]\n",
      " [ 0.30963365  0.24753413  0.24943293]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0]\n",
      "Episode finished after 487 timesteps\n",
      "[[ 0.47025549  0.42859459  0.35491168]\n",
      " [ 0.42730943  0.3254733   0.33187078]\n",
      " [ 0.4770062   0.36298363  0.35555664]\n",
      " [ 0.52979892  0.36460859  0.36444439]\n",
      " [ 0.51306208  0.41510811  0.51679042]\n",
      " [ 0.46484039  0.53102794  0.47469962]\n",
      " [ 0.44342924  0.49213052  0.43251142]\n",
      " [ 0.29486667  0.13773636  0.19055605]\n",
      " [ 0.19767321  0.22082237  0.18086322]\n",
      " [ 0.24597822  0.20735684  0.20230934]\n",
      " [ 0.25286986  0.25614088  0.28044966]\n",
      " [ 0.32099328  0.26509741  0.26681264]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0, 8.0]\n",
      "Episode finished after 467 timesteps\n",
      "[[ 0.4268591   0.49795609  0.41482904]\n",
      " [ 0.45329935  0.35904376  0.3503834 ]\n",
      " [ 0.54125873  0.36298363  0.35555664]\n",
      " [ 0.51562533  0.37917509  0.36444439]\n",
      " [ 0.47162357  0.44557748  0.47192543]\n",
      " [ 0.44101989  0.42921037  0.43079841]\n",
      " [ 0.43450276  0.43624228  0.41504776]\n",
      " [ 0.31994131  0.13773636  0.19055605]\n",
      " [ 0.2105375   0.22082237  0.18086322]\n",
      " [ 0.24863167  0.20735684  0.20230934]\n",
      " [ 0.26243195  0.26016896  0.28882637]\n",
      " [ 0.33688515  0.26509741  0.26681264]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0, 8.0, 8.0]\n",
      "Episode finished after 570 timesteps\n",
      "[[ 0.42847559  0.42364215  0.41482904]\n",
      " [ 0.45972772  0.35904376  0.36405834]\n",
      " [ 0.62659151  0.36298363  0.36640726]\n",
      " [ 0.56392675  0.44271311  0.53897212]\n",
      " [ 0.42591494  0.52604362  0.49772305]\n",
      " [ 0.45073187  0.50192502  0.42963013]\n",
      " [ 0.41507715  0.421297    0.41928159]\n",
      " [ 0.31994131  0.13773636  0.19055605]\n",
      " [ 0.2105375   0.22082237  0.18086322]\n",
      " [ 0.24863167  0.20735684  0.20230934]\n",
      " [ 0.26243195  0.26016896  0.28882637]\n",
      " [ 0.33688515  0.26509741  0.26681264]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0, 8.0, 8.0, 12.0]\n",
      "Episode finished after 471 timesteps\n",
      "[[ 0.42847559  0.42364215  0.41482904]\n",
      " [ 0.4795266   0.35904376  0.36405834]\n",
      " [ 0.60397059  0.36298363  0.36640726]\n",
      " [ 0.61688258  0.46187659  0.5402191 ]\n",
      " [ 0.55719444  0.54674749  0.55553009]\n",
      " [ 0.46749196  0.41789736  0.51017059]\n",
      " [ 0.41507715  0.42632328  0.41928159]\n",
      " [ 0.31994131  0.13773636  0.19055605]\n",
      " [ 0.2105375   0.22082237  0.18086322]\n",
      " [ 0.24863167  0.20735684  0.20230934]\n",
      " [ 0.26243195  0.26016896  0.28882637]\n",
      " [ 0.33688515  0.26509741  0.26681264]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0, 8.0, 8.0, 12.0, 8.0]\n",
      "Episode finished after 933 timesteps\n",
      "[[ 0.44827464  0.42407891  0.41587152]\n",
      " [ 0.49169552  0.35904376  0.36405834]\n",
      " [ 0.5947031   0.36298363  0.36640726]\n",
      " [ 0.60136912  0.46187659  0.5402191 ]\n",
      " [ 0.54338847  0.54387015  0.54388163]\n",
      " [ 0.55630777  0.48673752  0.49139323]\n",
      " [ 0.42363713  0.47250424  0.42326885]\n",
      " [ 0.31994131  0.13773636  0.19055605]\n",
      " [ 0.2105375   0.22082237  0.18086322]\n",
      " [ 0.24863167  0.20735684  0.20230934]\n",
      " [ 0.27375439  0.2836512   0.22764039]\n",
      " [ 0.36485549  0.2791626   0.26681264]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0, 8.0, 8.0, 12.0, 8.0, 17.0]\n",
      "Episode finished after 220 timesteps\n",
      "[[ 0.43290084  0.42494038  0.41755372]\n",
      " [ 0.4999676   0.35904376  0.36405834]\n",
      " [ 0.58454363  0.36298363  0.36640726]\n",
      " [ 0.58096828  0.46187659  0.5402191 ]\n",
      " [ 0.53949023  0.54231907  0.62863239]\n",
      " [ 0.5601823   0.48673752  0.49139323]\n",
      " [ 0.43168983  0.48077063  0.42326885]\n",
      " [ 0.31994131  0.13773636  0.19055605]\n",
      " [ 0.2105375   0.22082237  0.18086322]\n",
      " [ 0.24863167  0.20735684  0.20230934]\n",
      " [ 0.27375439  0.25057124  0.22764039]\n",
      " [ 0.38960256  0.29020949  0.29014428]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0, 8.0, 8.0, 12.0, 8.0, 17.0, 0.0]\n",
      "Episode finished after 314 timesteps\n",
      "[[ 0.4431187   0.42494038  0.41755372]\n",
      " [ 0.50493072  0.35904376  0.36405834]\n",
      " [ 0.57282592  0.36298363  0.36640726]\n",
      " [ 0.56619839  0.46187659  0.5402191 ]\n",
      " [ 0.62000811  0.52162682  0.52114782]\n",
      " [ 0.55884994  0.48673752  0.49139323]\n",
      " [ 0.43881734  0.49254368  0.42326885]\n",
      " [ 0.31994131  0.13773636  0.19055605]\n",
      " [ 0.23151552  0.31085336  0.20120917]\n",
      " [ 0.19129231  0.22341348  0.20850677]\n",
      " [ 0.28852048  0.25057124  0.22764039]\n",
      " [ 0.39052299  0.29020949  0.29014428]]\n",
      "[13.0, 15.0, 13.0, 11.0, 10.0, 14.0, 15.0, 13.0, 8.0, 8.0, 12.0, 8.0, 17.0, 0.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# this is for Q learning!!!\n",
    "epsilon = 0.9\n",
    "Q = np.zeros([12, 3])  # states, actions \n",
    "discount_factor = 0.9\n",
    "alpha = 0.1\n",
    "num_episodes = 500\n",
    "scores_list = []\n",
    "for i_episode in range(500):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    angle = 0\n",
    "    # the first state is based on the angular position of the hand\n",
    "    current_state = 1 # 1 because we always start facing the demons\n",
    "    \n",
    "    score = 0   \n",
    "    for t in itertools.count():\n",
    "        env.render()\n",
    "        # print(observation.shape)\n",
    "        \n",
    "        set_of_actions = ['right', 'left', 'shoot'] # index 0, 1, and 2\n",
    "        # decision process\n",
    "        # generate a random num between 0 and 1 e.g. 0.35, 0.73 etc..\n",
    "        # if the generated num is smaller than epsilon, we follow exploration policy \n",
    "        if np.random.random() <= epsilon:\n",
    "            # select a random action from set of all actions\n",
    "            index_action = set_of_actions.index(rd.choice(set_of_actions))\n",
    "            action = set_of_actions[index_action]\n",
    "        # if the generated num is greater than epsilon, we follow exploitation policy\n",
    "        else:\n",
    "            # select an action with highest value for current state\n",
    "            index_action = np.argmax(Q[current_state, :])\n",
    "            action = set_of_actions[index_action]\n",
    "        \n",
    "        # 'left'\n",
    "        actionVector = [0]*43\n",
    "        if action == 'left':\n",
    "            actionVector[15] = 1\n",
    "        elif action == 'right':\n",
    "            actionVector[14] = 1\n",
    "        elif action == 'shoot':\n",
    "            actionVector[0] = 1\n",
    "        else:\n",
    "            print(\"error in action\")\n",
    "        \n",
    "        # apply selected action, collect values for next_state and reward\n",
    "        observation, reward, done, info = env.step(actionVector)\n",
    "        \n",
    "        score = score+reward\n",
    "        next_states = np.array(next_possible_states(current_state))\n",
    "        \n",
    "        # Calculate the Q-learning target value\n",
    "        Q_target = reward + discount_factor*np.max(Q[next_states,:])\n",
    "        # Calculate the difference/error between target and current Q\n",
    "        Q_delta = Q_target - Q[current_state,index_action]\n",
    "        # Update the Q table, alpha is the learning rate\n",
    "        Q[current_state, index_action] = Q[current_state, index_action] + (alpha * Q_delta)\n",
    "        \n",
    "        \n",
    "        angle += actionToAngle(action)\n",
    "        if angle > 90:\n",
    "            angle = -87\n",
    "        if angle < -90:\n",
    "            angle = 87\n",
    "        current_state = angleToState(angle)\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(Q)\n",
    "            scores_list.append(score)\n",
    "            print(scores_list)\n",
    "            break\n",
    "            \n",
    "    # gradualy decay the epsilon\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= 1.0/num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1115 timesteps\n",
      "[[  7.37430241e-02  -2.74370155e-02   7.26507967e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   9.00000000e-02]\n",
      " [  1.11542756e-04   0.00000000e+00   1.05697006e-05]\n",
      " [  2.57535309e-02   2.66105133e-04   0.00000000e+00]\n",
      " [  1.17899316e-01   2.16273030e-03   6.53146316e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "Episode finished after 578 timesteps\n",
      "[[ 0.08905706  0.17216382 -0.01756979]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.09        0.          0.        ]\n",
      " [ 0.0113472   0.00708903  0.06529929]\n",
      " [ 0.00836259  0.06755796  0.00681918]\n",
      " [ 0.09969174  0.00761246  0.01106065]\n",
      " [ 0.09991292  0.01934497  0.06531463]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Episode finished after 1206 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.15224452  0.07963352  0.07024799]\n",
      " [ 0.08537748  0.04029487  0.07698444]\n",
      " [ 0.0828394   0.08098467  0.08697031]\n",
      " [ 0.04399316  0.10143687  0.0497076 ]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.13089745  0.07056049  0.15435187]\n",
      " [ 0.07893363  0.05428614  0.05774751]\n",
      " [ 0.0348553   0.03420363  0.10462383]\n",
      " [ 0.01747717  0.00181331  0.04648545]\n",
      " [ 0.01221578  0.03687947  0.00529419]]\n",
      "Episode finished after 513 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.13240706  0.16314966  0.16455781]\n",
      " [ 0.08152761  0.13191069  0.10308009]\n",
      " [ 0.07480048  0.09413113  0.08270544]\n",
      " [ 0.04844432  0.18490841  0.05386616]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.04389755  0.14171568  0.14578784]\n",
      " [ 0.19745213  0.05732046  0.09075922]\n",
      " [ 0.05874249  0.03465913  0.09907157]\n",
      " [ 0.02024352  0.00181331  0.04340985]\n",
      " [ 0.01221578  0.03687947  0.00529419]]\n",
      "Episode finished after 495 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.17223554  0.17155088  0.17095577]\n",
      " [ 0.08307368  0.13454496  0.1060551 ]\n",
      " [ 0.07661137  0.09084752  0.08270544]\n",
      " [ 0.04844432  0.18490841  0.05386616]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.1360742   0.19683188  0.11706392]\n",
      " [ 0.16506824  0.13396951  0.05917646]\n",
      " [ 0.13267006  0.06350938  0.09566514]\n",
      " [ 0.10297415  0.00603455  0.05917302]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 386 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.16217751  0.16175308  0.05557444]\n",
      " [ 0.09608306  0.18781843  0.14361364]\n",
      " [ 0.07661137  0.09084752  0.08270544]\n",
      " [ 0.04844432  0.18490841  0.05386616]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.15306668  0.1521975   0.22091904]\n",
      " [ 0.18801767  0.13706801  0.10758054]\n",
      " [ 0.15001787  0.06585453  0.09566514]\n",
      " [ 0.10470528  0.00603455  0.05917302]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 704 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.06275335  0.16566805  0.15795648]\n",
      " [ 0.15033113  0.15545235  0.16021078]\n",
      " [ 0.13252332  0.20500536  0.13477056]\n",
      " [ 0.04844432  0.18246333  0.05386616]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.15203012  0.16095229  0.20931459]\n",
      " [ 0.18047829  0.15127402  0.13626213]\n",
      " [ 0.15001787  0.06585453  0.09566514]\n",
      " [ 0.10470528  0.00603455  0.05917302]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 776 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.09321962  0.15288761  0.14106298]\n",
      " [ 0.15033113  0.15545235  0.16021078]\n",
      " [ 0.13252332  0.20500536  0.13477056]\n",
      " [ 0.04844432  0.18246333  0.05386616]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.14561246  0.0599279   0.14600755]\n",
      " [ 0.17262585  0.1739864   0.21039274]\n",
      " [ 0.27949577  0.1260153   0.16801261]\n",
      " [ 0.12762625  0.03357419  0.06604058]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 426 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.25790883  0.14815724  0.13959591]\n",
      " [ 0.17709162  0.17963724  0.1829126 ]\n",
      " [ 0.02901749  0.15356739  0.14690012]\n",
      " [ 0.04844432  0.16805373  0.07322318]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.12466437  0.07530085  0.12623258]\n",
      " [ 0.16738961  0.1739864   0.21039274]\n",
      " [ 0.27949577  0.1260153   0.16801261]\n",
      " [ 0.12762625  0.03357419  0.06604058]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 847 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.1557795   0.14642756  0.15582497]\n",
      " [ 0.1622712   0.16528527  0.16635463]\n",
      " [ 0.07442545  0.16220997  0.13670691]\n",
      " [ 0.04844432  0.16805373  0.07322318]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.13897038  0.06540274  0.13705946]\n",
      " [ 0.15879817  0.16096611  0.15958635]\n",
      " [ 0.17574362  0.16109694  0.17586435]\n",
      " [ 0.12762625  0.03357419  0.06604058]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 680 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.1885817   0.1637365   0.15059397]\n",
      " [ 0.15181276  0.16619036  0.15622924]\n",
      " [ 0.13805241  0.15076942  0.14871863]\n",
      " [ 0.04844432  0.16170635  0.08732656]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.14529637  0.08978819  0.12762303]\n",
      " [ 0.15879817  0.16096611  0.15958635]\n",
      " [ 0.17574362  0.16109694  0.17586435]\n",
      " [ 0.12762625  0.03357419  0.06604058]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 1033 timesteps\n",
      "[[ 0.04557892  0.04619738 -0.05423854]\n",
      " [ 0.12169534  0.13160407  0.12742675]\n",
      " [ 0.1330283   0.14373207  0.1414116 ]\n",
      " [ 0.15093435  0.13451225  0.15028424]\n",
      " [ 0.05794935  0.24659432  0.08732656]\n",
      " [ 0.06720601  0.04101739  0.01613676]\n",
      " [ 0.10424351  0.02366415  0.06037335]\n",
      " [ 0.14657321  0.15043001  0.04765272]\n",
      " [ 0.14315666  0.14396498  0.1434263 ]\n",
      " [ 0.14614716  0.14203845  0.14575264]\n",
      " [ 0.11956344  0.04820834  0.06604058]\n",
      " [ 0.0155108   0.03687947  0.00529419]]\n",
      "Episode finished after 491 timesteps\n",
      "[[ 0.0312494   0.06582568  0.01091028]\n",
      " [ 0.12033552  0.11290313  0.12742675]\n",
      " [ 0.1330283   0.14373207  0.1414116 ]\n",
      " [ 0.15093435  0.13451225  0.15028424]\n",
      " [ 0.04687059  0.24659432  0.08732656]\n",
      " [ 0.04846397 -0.05514908  0.03936968]\n",
      " [ 0.04473933  0.08906092  0.06033177]\n",
      " [ 0.13276893  0.13196853  0.07400791]\n",
      " [ 0.12439195  0.12748134  0.12517723]\n",
      " [ 0.12915917  0.14675738  0.13354758]\n",
      " [ 0.20557032  0.19091654  0.07026403]\n",
      " [ 0.04995056  0.03725604  0.00529419]]\n",
      "Episode finished after 632 timesteps\n",
      "[[ 0.0312494   0.06582568  0.01091028]\n",
      " [ 0.17300198  0.06911793  0.1365497 ]\n",
      " [ 0.13124629  0.14268162  0.13368856]\n",
      " [ 0.15093435  0.13451225  0.15028424]\n",
      " [ 0.04687059  0.24659432  0.08732656]\n",
      " [ 0.04846397 -0.05514908  0.03936968]\n",
      " [ 0.04473933  0.08906092  0.06033177]\n",
      " [ 0.15536173  0.12426984  0.0883528 ]\n",
      " [ 0.10176225  0.12748134  0.12517723]\n",
      " [ 0.12915917  0.14675738  0.13354758]\n",
      " [ 0.20557032  0.19091654  0.07026403]\n",
      " [ 0.04995056  0.03725604  0.00529419]]\n",
      "Episode finished after 750 timesteps\n",
      "[[ 0.0312494   0.06582568  0.01091028]\n",
      " [ 0.12966928  0.12804246  0.13039875]\n",
      " [ 0.12795393  0.02742351  0.12679767]\n",
      " [ 0.1324297   0.13324029  0.13605255]\n",
      " [ 0.1273301   0.14960259  0.09801173]\n",
      " [ 0.04846397 -0.05514908  0.03936968]\n",
      " [ 0.04473933  0.08906092  0.06033177]\n",
      " [ 0.13122067  0.12526599  0.09434652]\n",
      " [ 0.10176225  0.12748134  0.12517723]\n",
      " [ 0.12915917  0.14675738  0.13354758]\n",
      " [ 0.20557032  0.19091654  0.07026403]\n",
      " [ 0.04995056  0.03725604  0.00529419]]\n"
     ]
    }
   ],
   "source": [
    "# this is for SARSA learning!!!\n",
    "epsilon = 0.9\n",
    "Q = np.zeros([12, 3])  # states, actions \n",
    "discount_factor = 0.9\n",
    "alpha = 0.1\n",
    "num_episodes = 15\n",
    "scores_list = []\n",
    "for i_episode in range(15):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    angle = 0\n",
    "    # the first state is based on the angular position of the hand\n",
    "    current_state = 1 # 1 because we always start facing the demons\n",
    "    # random action\n",
    "    set_of_actions = ['right', 'left', 'shoot'] # index 0, 1, and 2\n",
    "    current_index_action = set_of_actions.index(rd.choice(set_of_actions))\n",
    "    action = set_of_actions[current_index_action]\n",
    "    \n",
    "    score = 0\n",
    "    for t in itertools.count():\n",
    "        env.render()\n",
    "        # print(observation.shape)      \n",
    "        \n",
    "       \n",
    "        angle += actionToAngle(action)\n",
    "        if angle > 90:\n",
    "            angle = -87\n",
    "        if angle < -90:\n",
    "            angle = 87\n",
    "        next_state = angleToState(angle)\n",
    "         \n",
    "        # decision process\n",
    "        # generate a random num between 0 and 1 e.g. 0.35, 0.73 etc..\n",
    "        # if the generated num is smaller than epsilon, we follow exploration policy \n",
    "        if np.random.random() <= epsilon:\n",
    "            # select a random action from set of all actions\n",
    "            next_index_action = set_of_actions.index(rd.choice(set_of_actions))\n",
    "            next_action = set_of_actions[next_index_action]\n",
    "        # if the generated num is greater than epsilon, we follow exploitation policy\n",
    "        else:\n",
    "            # select an action with highest value for current state\n",
    "            next_index_action = np.argmax(Q[current_state, :])\n",
    "            next_action = set_of_actions[next_index_action]\n",
    "        \n",
    "        # 'left'\n",
    "        actionVector = [0]*43\n",
    "        if action == 'left':\n",
    "            actionVector[15] = 1\n",
    "        elif action == 'right':\n",
    "            actionVector[14] = 1\n",
    "        elif action == 'shoot':\n",
    "            actionVector[0] = 1\n",
    "        else:\n",
    "            print(\"error in action\")\n",
    "        \n",
    "        # apply selected action, collect values for next_state and reward\n",
    "        observation, reward, done, info = env.step(actionVector)\n",
    "        score = score + reward\n",
    "        \n",
    "        # Calculate the Q-learning target value\n",
    "        Q_target = reward + discount_factor*(Q[next_state, next_index_action])\n",
    "        # Calculate the difference/error between target and current Q\n",
    "        Q_delta = Q_target - Q[current_state,current_index_action]\n",
    "        # Update the Q table, alpha is the learning rate\n",
    "        Q[current_state, current_index_action] = Q[current_state, current_index_action] + (alpha * Q_delta)\n",
    "        \n",
    "        \n",
    "        current_state = next_state\n",
    "        action = next_action\n",
    "        current_index_action = next_index_action\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(Q)\n",
    "            scores_list.append(score)\n",
    "            break\n",
    "            \n",
    "    # gradualy decay the epsilon\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= 1.0/num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.0,\n",
       " 9.0,\n",
       " 21.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 17.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 15.0,\n",
       " 6.0,\n",
       " 12.0,\n",
       " 12.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "[7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[7, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
      "[8, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "[8, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "[8, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
      "[7, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "[7, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "[2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "[2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "[3, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "[3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[4, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "[4, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "[5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "[6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]\n",
      "[0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]\n",
      "[0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "[0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "[11, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "[10, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "[10, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0]\n",
      "[9, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[9, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[8, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[8, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[7, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "[2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[4, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0]\n",
      "[4, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[5, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[5, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[6, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# THIS IS FOR 10 FRAMES!!!\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    angle = 0\n",
    "    current_state = [0]*11 # our state is a vector of 11 elements\n",
    "    # the first state is based on the angular position of the hand\n",
    "    current_state[0] = 1 # 1 because we always start looking straight to the demons\n",
    "    \n",
    "    \n",
    "    for t in itertools.count():\n",
    "        # generate a random num between 0 and 1 e.g. 0.35, 0.73 etc..\n",
    "        # if the generated num is smaller than epsilon, we follow exploration policy \n",
    "        if np.random.random() <= epsilon:\n",
    "            # select a random action from set of all actions\n",
    "            action = choice(actions)\n",
    "        # if the generated num is greater than epsilon, we follow exploitation policy\n",
    "        else:\n",
    "            # select an action with highest value for current state\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "        # apply selected action, collect values for next_state and reward\n",
    "        reward = game.make_action(action)\n",
    "        next_state,  done, _ = env.step(action)\n",
    "\n",
    "        # Calculate the Q-learning target value\n",
    "        Q_target = reward + discount_factor*np.max(Q[next_state,:])\n",
    "        # Calculate the difference/error between target and current Q\n",
    "        Q_delta = Q_target - Q[state,action]\n",
    "        # Update the Q table, alpha is the learning rate\n",
    "        Q[state, action] = Q[state, action] + (alpha * Q_delta)\n",
    "\n",
    "        # break if done, i.e. if end of this episode\n",
    "        if done:\n",
    "            break\n",
    "        # make the next_state into current state as we go for next iteration\n",
    "        state = next_state\n",
    "    # gradualy decay the epsilon\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= 1.0/num_episodes\n",
    "    \n",
    "    \n",
    "    Q = np.zeros([11, 3])  # states, actions \n",
    "    \n",
    "    for t in itertools.count():\n",
    "        env.render()\n",
    "        # print(observation.shape)\n",
    "        \n",
    "        \n",
    "        # decision process\n",
    "        # generate a random num between 0 and 1 e.g. 0.35, 0.73 etc..\n",
    "        # if the generated num is smaller than epsilon, we follow exploration policy \n",
    "        if np.random.random() <= epsilon:\n",
    "            # select a random action from set of all actions\n",
    "            action = choice(actions)\n",
    "        # if the generated num is greater than epsilon, we follow exploitation policy\n",
    "        else:\n",
    "            # select an action with highest value for current state\n",
    "            action = np.argmax(Q[state, :])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 'left'\n",
    "        action = [0]*43\n",
    "        if t < 5:\n",
    "            actionAsString = 'left'\n",
    "            action[14] = 1\n",
    "        else:\n",
    "            actionAsString = 'right'\n",
    "            action[15] = 1\n",
    "        \n",
    "       \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        frames = observation[188:235, 40:600]\n",
    "        frame1 = frames[:, 1:54]\n",
    "        frame2 = frames[:, 55:109]\n",
    "        frame3 = frames[:, 110:164]\n",
    "        frame4 = frames[:, 165:219]\n",
    "        frame5 = frames[:, 220:274]\n",
    "        frame6 = frames[:, 274:329]\n",
    "        frame7 = frames[:, 330:384]\n",
    "        frame8 = frames[:, 385:439]\n",
    "        frame9 = frames[:, 440:494]\n",
    "        frame10 = frames[:, 495:560]\n",
    "    \n",
    "        rgb_frame1 = np.mean(frame1, axis=(0, 1))\n",
    "        rgb_frame2 = np.mean(frame2, axis=(0, 1))\n",
    "        rgb_frame3 = np.mean(frame3, axis=(0, 1))\n",
    "        rgb_frame4 = np.mean(frame4, axis=(0, 1))\n",
    "        rgb_frame5 = np.mean(frame5, axis=(0, 1))\n",
    "        rgb_frame6 = np.mean(frame6, axis=(0, 1))\n",
    "        rgb_frame7 = np.mean(frame7, axis=(0, 1))\n",
    "        rgb_frame8 = np.mean(frame8, axis=(0, 1))\n",
    "        rgb_frame9 = np.mean(frame9, axis=(0, 1))\n",
    "        rgb_frame10 = np.mean(frame10, axis=(0, 1))\n",
    "        all_frame_avg = [rgb_frame1, rgb_frame2, rgb_frame3, rgb_frame4, rgb_frame5, rgb_frame6, \n",
    "                         rgb_frame7, rgb_frame8, rgb_frame9, rgb_frame10]\n",
    "        \n",
    "#         print(frame1.shape)\n",
    "#         print(rgb_frame1)\n",
    "#         print(rgb_frame2)\n",
    "#         print(rgb_frame3)\n",
    "#         print(rgb_frame4)\n",
    "#         print(rgb_frame5)\n",
    "#         print(rgb_frame6)\n",
    "#         print(rgb_frame7)\n",
    "#         print(rgb_frame8)\n",
    "#         print(rgb_frame9)\n",
    "#         print(rgb_frame10)\n",
    "        \n",
    "        angle += actionToAngle(actionAsString)\n",
    "        if angle > 90:\n",
    "            angle = -87\n",
    "        if angle < -90:\n",
    "            angle = 87\n",
    "        current_state = []\n",
    "        current_state.append(angleToState(angle))\n",
    "        current_state= current_state + [frameToFind(f) for f in all_frame_avg]\n",
    "        print(current_state)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAD8CAYAAABO8KDVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88c44e0f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 109.90937746   49.97557132   43.3356974 ]\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(frame8)\n",
    "plt.show()\n",
    "print(rgb_frame3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = observation[210:270, :]\n",
    "frame1 = frames[:, 1:128]\n",
    "frame2 = frames[:, 129:256]\n",
    "frame3 = frames[:, 257:384]\n",
    "frame4 = frames[:, 384:512]\n",
    "frame5 = frames[:. 513:640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THIS IS FOR 10 FRAMES!!!\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(30):\n",
    "        env.render()\n",
    "        # print(observation.shape)\n",
    "        action = [0]*43\n",
    "        action[14] = 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -2,\n",
       " 2,\n",
       " -97,\n",
       " -40,\n",
       " 32]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(480, 640, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning(game, num_episodes, alpha=0.85, discount_factor=0.99):\n",
    "    \"\"\"\n",
    "    Q learning algorithm, off-polics TD control. Finds optimal gready policies\n",
    "    Args:\n",
    "    env: Given environment to solve\n",
    "    num_episodes: Number of episodes to learn\n",
    "    alpha: learning rate\n",
    "    discount factor: weight/importance given to future rewards\n",
    "    epsilon: probability of taking random action. \n",
    "             We are using decaying epsilon, \n",
    "             i.e high randomness at beginning and low towards end\n",
    "    Returns:\n",
    "    Optimal Q\n",
    "    \"\"\"\n",
    "     \n",
    "    # decaying epsilon, i.e we will divide num of episodes passed\n",
    "    epsilon = 1.0\n",
    "    # create a numpy array filled with zeros \n",
    "    # rows = number of observations & cols = possible actions\n",
    "    actions = []\n",
    "    actions_num = game.get_available_buttons_size()\n",
    "    Q = np.zeros([env.observation_space.shape[0], actions_num]) \n",
    "    for perm in it.product([False, True], repeat=actions_num):\n",
    "        actions.append(list(perm))\n",
    "    for i_episode in range(num_episodes):\n",
    "            # reset the env\n",
    "            state = game.get_state()\n",
    "            # itertools.count() has similar to 'while True:'\n",
    "            for t in itertools.count():\n",
    "                # generate a random num between 0 and 1 e.g. 0.35, 0.73 etc..\n",
    "                # if the generated num is smaller than epsilon, we follow exploration policy \n",
    "                if np.random.random() <= epsilon:\n",
    "                    # select a random action from set of all actions\n",
    "                    action = choice(actions)\n",
    "                # if the generated num is greater than epsilon, we follow exploitation policy\n",
    "                else:\n",
    "                    # select an action with highest value for current state\n",
    "                    action = np.argmax(Q[state, :])\n",
    "                \n",
    "                # apply selected action, collect values for next_state and reward\n",
    "                reward = game.make_action(action)\n",
    "                next_state,  done, _ = env.step(action)\n",
    "                \n",
    "                # Calculate the Q-learning target value\n",
    "                Q_target = reward + discount_factor*np.max(Q[next_state,:])\n",
    "                # Calculate the difference/error between target and current Q\n",
    "                Q_delta = Q_target - Q[state,action]\n",
    "                # Update the Q table, alpha is the learning rate\n",
    "                Q[state, action] = Q[state, action] + (alpha * Q_delta)\n",
    "                \n",
    "                # break if done, i.e. if end of this episode\n",
    "                if done:\n",
    "                    break\n",
    "                # make the next_state into current state as we go for next iteration\n",
    "                state = next_state\n",
    "            # gradualy decay the epsilon\n",
    "            if epsilon > 0.1:\n",
    "                epsilon -= 1.0/num_episodes\n",
    "    \n",
    "    return Q    # return optimal Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (480,640,3) (43,) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9beaaea301f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-ae0929b493a2>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, num_episodes, alpha, discount_factor)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mQ_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Calculate the difference/error between target and current Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mQ_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_target\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# Update the Q table, alpha is the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (480,640,3) (43,) "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "Q = q_learning(env, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1\n",
      "State #1\n",
      "Game Variables: [  79.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #2\n",
      "Game Variables: [  80.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #3\n",
      "Game Variables: [  81.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #4\n",
      "Game Variables: [  82.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #5\n",
      "Game Variables: [  83.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #6\n",
      "Game Variables: [  84.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #7\n",
      "Game Variables: [  85.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #8\n",
      "Game Variables: [  86.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #9\n",
      "Game Variables: [  87.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 1.0\n",
      "=====================\n",
      "State #10\n",
      "Game Variables: [  87.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #11\n",
      "Game Variables: [  88.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #12\n",
      "Game Variables: [  89.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #13\n",
      "Game Variables: [  90.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #14\n",
      "Game Variables: [  91.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #15\n",
      "Game Variables: [  92.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #16\n",
      "Game Variables: [  93.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #17\n",
      "Game Variables: [  94.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #18\n",
      "Game Variables: [  95.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #19\n",
      "Game Variables: [  96.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #20\n",
      "Game Variables: [  97.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #21\n",
      "Game Variables: [  98.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #22\n",
      "Game Variables: [  99.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #23\n",
      "Game Variables: [ 100.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #24\n",
      "Game Variables: [ 101.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #25\n",
      "Game Variables: [ 102.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #26\n",
      "Game Variables: [ 103.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #27\n",
      "Game Variables: [ 104.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #28\n",
      "Game Variables: [ 105.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #29\n",
      "Game Variables: [ 105.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #30\n",
      "Game Variables: [ 106.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #31\n",
      "Game Variables: [ 107.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #32\n",
      "Game Variables: [ 108.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #33\n",
      "Game Variables: [ 109.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #34\n",
      "Game Variables: [ 110.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #35\n",
      "Game Variables: [ 111.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #36\n",
      "Game Variables: [ 112.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #37\n",
      "Game Variables: [ 113.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #38\n",
      "Game Variables: [ 114.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #39\n",
      "Game Variables: [ 115.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #40\n",
      "Game Variables: [ 116.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #41\n",
      "Game Variables: [ 117.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #42\n",
      "Game Variables: [ 118.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #43\n",
      "Game Variables: [ 118.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #44\n",
      "Game Variables: [ 119.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #45\n",
      "Game Variables: [ 120.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #46\n",
      "Game Variables: [ 121.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #47\n",
      "Game Variables: [ 122.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #48\n",
      "Game Variables: [ 123.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #49\n",
      "Game Variables: [ 124.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #50\n",
      "Game Variables: [ 125.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #51\n",
      "Game Variables: [ 126.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #52\n",
      "Game Variables: [ 127.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #53\n",
      "Game Variables: [ 128.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #54\n",
      "Game Variables: [ 129.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #55\n",
      "Game Variables: [ 130.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #56\n",
      "Game Variables: [ 131.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 1.0\n",
      "=====================\n",
      "State #57\n",
      "Game Variables: [ 131.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #58\n",
      "Game Variables: [ 132.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #59\n",
      "Game Variables: [ 133.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #60\n",
      "Game Variables: [ 134.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #61\n",
      "Game Variables: [ 135.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #62\n",
      "Game Variables: [ 136.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #63\n",
      "Game Variables: [ 137.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #64\n",
      "Game Variables: [ 138.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #65\n",
      "Game Variables: [ 139.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #66\n",
      "Game Variables: [ 140.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #67\n",
      "Game Variables: [ 141.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #68\n",
      "Game Variables: [ 142.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #69\n",
      "Game Variables: [ 143.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #70\n",
      "Game Variables: [ 144.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 1.0\n",
      "=====================\n",
      "State #71\n",
      "Game Variables: [ 144.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #72\n",
      "Game Variables: [ 145.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #73\n",
      "Game Variables: [ 146.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #74\n",
      "Game Variables: [ 147.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #75\n",
      "Game Variables: [ 148.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #76\n",
      "Game Variables: [ 149.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State #77\n",
      "Game Variables: [ 150.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #78\n",
      "Game Variables: [ 151.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #79\n",
      "Game Variables: [ 152.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #80\n",
      "Game Variables: [ 153.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #81\n",
      "Game Variables: [ 154.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #82\n",
      "Game Variables: [ 155.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #83\n",
      "Game Variables: [ 156.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #84\n",
      "Game Variables: [ 157.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #85\n",
      "Game Variables: [ 157.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #86\n",
      "Game Variables: [ 158.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #87\n",
      "Game Variables: [ 159.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #88\n",
      "Game Variables: [ 160.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #89\n",
      "Game Variables: [ 161.  100.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #90\n",
      "Game Variables: [ 162.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #91\n",
      "Game Variables: [ 163.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #92\n",
      "Game Variables: [ 164.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #93\n",
      "Game Variables: [ 165.  100.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #94\n",
      "Game Variables: [ 166.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #95\n",
      "Game Variables: [ 167.  100.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #96\n",
      "Game Variables: [ 168.  100.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #97\n",
      "Game Variables: [ 169.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #98\n",
      "Game Variables: [ 170.  100.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #99\n",
      "Game Variables: [ 170.  100.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #100\n",
      "Game Variables: [ 171.  100.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #101\n",
      "Game Variables: [ 172.  100.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #102\n",
      "Game Variables: [ 173.   91.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #103\n",
      "Game Variables: [ 174.   91.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #104\n",
      "Game Variables: [ 175.   91.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #105\n",
      "Game Variables: [ 176.   91.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #106\n",
      "Game Variables: [ 177.   91.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #107\n",
      "Game Variables: [ 178.   91.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #108\n",
      "Game Variables: [ 179.   91.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #109\n",
      "Game Variables: [ 180.   91.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #110\n",
      "Game Variables: [ 181.   91.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #111\n",
      "Game Variables: [ 182.   91.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #112\n",
      "Game Variables: [ 183.   91.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #113\n",
      "Game Variables: [ 183.   91.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #114\n",
      "Game Variables: [ 184.   91.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #115\n",
      "Game Variables: [ 185.   91.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #116\n",
      "Game Variables: [ 186.   91.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #117\n",
      "Game Variables: [ 187.   91.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #118\n",
      "Game Variables: [ 188.   88.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #119\n",
      "Game Variables: [ 189.   88.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #120\n",
      "Game Variables: [ 190.   88.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #121\n",
      "Game Variables: [ 191.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #122\n",
      "Game Variables: [ 192.   88.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #123\n",
      "Game Variables: [ 193.   88.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #124\n",
      "Game Variables: [ 194.   88.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #125\n",
      "Game Variables: [ 195.   88.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #126\n",
      "Game Variables: [ 196.   88.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #127\n",
      "Game Variables: [ 197.   88.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #128\n",
      "Game Variables: [ 198.   88.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #129\n",
      "Game Variables: [ 199.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #130\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #131\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 1.0\n",
      "=====================\n",
      "State #132\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #133\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #134\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #135\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #136\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #137\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #138\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #139\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #140\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #141\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #142\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #143\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #144\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #145\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #146\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #147\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #148\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #149\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #150\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #151\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State #152\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #153\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #154\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 1.0]\n",
      "Last Reward: 1.0\n",
      "=====================\n",
      "State #155\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #156\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #157\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #158\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #159\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 1.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #160\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [1.0, 0.0, 0.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #161\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 0.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n",
      "State #162\n",
      "Game Variables: [ 200.   88.]\n",
      "Performed action: [0.0, 1.0, 1.0]\n",
      "Last Reward: 0.0\n",
      "=====================\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-774dcfb280b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Makes a random action and save the reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State #\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools as it\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "import time\n",
    "\n",
    "game = DoomGame()\n",
    "\n",
    "game.load_config(\"defend_the_line.cfg\")\n",
    "game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "game.set_window_visible(True)\n",
    "game.init()\n",
    "\n",
    "# Creates all possible actions depending on how many buttons there are.\n",
    "actions_num = game.get_available_buttons_size()\n",
    "actions = []\n",
    "for perm in it.product([False, True], repeat=actions_num):\n",
    "    actions.append(list(perm))\n",
    "\n",
    "episodes = 10\n",
    "sleep_time = 0.028\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(\"Episode #\" + str(i + 1))\n",
    "\n",
    "    # Not needed for the first episode but the loop is nicer.\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "\n",
    "        # Gets the state and possibly to something with it\n",
    "        state = game.get_state()\n",
    "\n",
    "        # Makes a random action and save the reward.\n",
    "        reward = game.make_action(choice(actions))\n",
    "\n",
    "        print(\"State #\" + str(state.number))\n",
    "        print(\"Game Variables:\", state.game_variables)\n",
    "        print(\"Performed action:\", game.get_last_action())\n",
    "        print(\"Last Reward:\", reward)\n",
    "        print(\"=====================\")\n",
    "\n",
    "        # Sleep some time because processing is too fast to watch.\n",
    "        if sleep_time > 0:\n",
    "            sleep(sleep_time)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"total reward:\", game.get_total_reward())\n",
    "print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.set_death_penalty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_num = game.get_available_buttons_size()\n",
    "actions_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " game.get_last_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
